# D²TL: Algorithms, Formulas, Experiment Design & Narrative

**Document Version:** 1.0  
**Author:** NOK KO  
**Project:** Model 3 — Physics-Aware Dual-Path 6G RSU Coverage Predictor (D²TL)  
**Last Updated:** 2026-02

This document consolidates **all algorithm formulas** (200+), **model backbones**, **Selector logic**, **experiment design and plan**, and the **storyline** of the D²TL system in one place. It is intended for reproducibility, paper writing, and implementation reference.

---

## Table of Contents

1. [Story & Narrative](#1-story--narrative)
2. [Part I: Physics & Data Generation](#2-part-i-physics--data-generation)
3. [Part II: MLP Backbone](#3-part-ii-mlp-backbone)
4. [Part III: Mamba-3 Backbone](#4-part-iii-mamba-3-backbone)
5. [Part IV: Selector Brain](#5-part-iv-selector-brain)
6. [Part V: Training](#6-part-v-training)
7. [Part VI: Experiment Design & Plan](#7-part-vi-experiment-design--plan)
8. [Part VII: Summary Tables](#8-part-vii-summary-tables)
9. [Appendix A: Complete Formula List (F1–F212)](#9-appendix-a-complete-formula-list-f1f212)
10. [Appendix B: Experiment Protocol](#10-appendix-b-experiment-protocol-step-by-step)
11. [Appendix C: Algorithm Pseudocode](#11-appendix-c-algorithm-pseudocode)
12. [Appendix D: Data Schema and API](#12-appendix-d-data-schema-and-api-contract)
13. [Appendix E: Backbone Dimensions](#13-appendix-e-backbone-dimensions-layer-by-layer)
14. [Appendix F: Constants Quick Reference](#14-appendix-f-constants-quick-reference)
15. [Storyline (Extended)](#15-storyline-extended)
16. [Appendix G: Formula Derivations & Notes](#16-appendix-g-formula-derivations--notes)
17. [Appendix H: Code-to-Formula Cross-Reference](#17-appendix-h-code-to-formula-cross-reference)
18. [Appendix I: Experiment Results JSON Schema](#18-appendix-i-experiment-results-json-schema)
19. [Appendix J: Narrative Timeline](#19-appendix-j-narrative-timeline-chronological)
20. [Appendix K: Summary Checklist for Paper](#20-appendix-k-summary-checklist-for-paper--report)
21. [Appendix L: Expected Outcomes and Interpretation](#21-appendix-l-expected-outcomes-and-interpretation)
22. [Appendix M: Symbol and Term Glossary](#22-appendix-m-symbol-and-term-glossary)

---

## 1. Story & Narrative

### 1.1 Motivation

6G RSU (Road Side Unit) coverage prediction must satisfy two conflicting demands: **low latency** for real-time control and **physical consistency** under extreme conditions (long range, heavy rain, dense urban). A single model either sacrifices accuracy under extrapolation (e.g. MLP) or is too slow for deployment (e.g. large Mamba).

### 1.2 D²TL Concept

**D²TL** (Dual-Path with Physics-Aware Trigger Logic) uses:

- **Primary path:** A fast MLP (~1–7 ms) that handles the majority of “normal” scenarios with high R².
- **Backup path:** A Mamba-3 state-space model that learns Friis path-loss slope, ITU-R rain attenuation, and density scaling — activated only when physics is “extreme.”
- **Selector Brain:** A rule-based + divergence-based orchestrator that decides per request whether to return MLP or Mamba output.

The **storyline** is: *Use the fast path by default; switch to the physics-aware path when distance, weather, density, or model disagreement indicate risk.*

### 1.3 Narrative Flow

1. **Data:** A physics-based generator produces 30,000 samples with path loss, antenna gain, weather attenuation, interference, SINR, coverage radius/area, and QoS.
2. **Training:** MLP and Mamba-3 are trained on the same dataset with the same normalization and multi-task loss; both output 5 targets (Power, SINR, Radius, Area, QoS).
3. **Selector:** At inference, the Selector computes a **trigger score** from distance, weather, density, and interferers; it also checks **divergence** between MLP and Mamba. If score ≥ 0.3 or (divergent and score ≥ 0.15), it returns Mamba; otherwise MLP.
4. **Experiments:** Seven experiments validate distribution (Exp1), distance–power slope (Exp2), rainstorm behavior (Exp3), stratified performance (Exp4), cost/latency (Exp5), ablation (Exp6), and tail risk (Exp7).

---

## 2. Part I: Physics & Data Generation

The training data is generated by **CoverageDataGeneratorV2**, which implements the following physics.

### 2.1 Constants

| Symbol | Value | Meaning |
|--------|--------|---------|
| \( f_c \) | 5.9 GHz | Carrier frequency |
| \( P_{\mathrm{tx}} \) | 33 dBm | Nominal transmit power |
| \( N_0 \) | -95 dBm | Noise floor |
| \( B \) | 10 MHz | Bandwidth |
| \( n \) | 3.5 | Path-loss exponent (free-space-like base) |
| \( d_0 \) | 1.0 m | Reference distance |
| \( \sigma_{\mathrm{sh}} \) | 8.0 dB | Shadowing standard deviation |
| \( H_{\mathrm{ant}} \) | 8.0 m | Antenna height |
| \( G_{\max} \) | 10.0 dBi | Maximum antenna gain |
| \( \theta_{\mathrm{bw}} \) | 120° | Antenna beamwidth |
| \( P_{\min} \) | -90 dBm | Minimum RX power (sensitivity) |
| \( \gamma_{\min} \) | -5 dB | Minimum SINR for coverage |

### 2.2 Wavelength and Free-Space Path Loss at Reference

**(F1)**  
\[
\lambda = \frac{c}{f_c} = \frac{3\times 10^8}{5.9\times 10^9} \approx 0.0508\;\text{m}.
\]

**(F2)** Free-space path loss at \(d_0 = 1\) m:
\[
L_0 = 20\,\log_{10}\left( \frac{4\pi d_0}{\lambda} \right)\;\text{dB}.
\]

**(F3)** With \(d_0=1\), \(\lambda\approx 0.0508\):
\[
L_0 = 20\,\log_{10}(4\pi/\lambda) \approx 20\,\log_{10}(247.2) \approx 47.86\;\text{dB}.
\]

### 2.3 Obstacle and Weather Factors

**(F4)** Obstacle factor by building density \(k \in \{0,1,2,3\}\) (Rural, Suburban, Urban, Ultra-Dense):
\[
\eta_{\mathrm{obs}}(k) = \begin{cases} 1.0 & k=0 \\ 1.2 & k=1 \\ 1.5 & k=2 \\ 2.0 & k=3 \end{cases}.
\]

**(F5)** Effective path-loss exponent:
\[
n_{\mathrm{eff}} = n \cdot \eta_{\mathrm{obs}}(k).
\]

**(F6)** Weather attenuation (dB) by weather condition \(w \in \{0,1,2,3\}\) (Clear, Light Rain, Moderate Rain, Heavy Rain):
\[
A_{\mathrm{weather}}(w) = \begin{cases} 0 & w=0 \\ 2 & w=1 \\ 5 & w=2 \\ 8 & w=3 \end{cases}\;\text{dB}.
\]

### 2.4 Path Loss (Log-Distance + Shadowing)

**(F7)** Log-distance path loss (dB):
\[
\mathrm{PL}_{\mathrm{log}}(d) = L_0 + 10\,n_{\mathrm{eff}}\,\log_{10}\left( \frac{d}{d_0} \right),\quad d \ge d_0.
\]

**(F8)** Shadowing (Gaussian):
\[
X_{\sigma} \sim \mathcal{N}(0,\, \sigma_{\mathrm{sh}}^2).
\]

**(F9)** Total path loss (dB):
\[
\mathrm{PL}(d) = \mathrm{PL}_{\mathrm{log}}(d) + X_{\sigma},\quad \mathrm{PL}(d) \ge 40\;\text{dB}.
\]

### 2.5 Antenna Gain

**(F10)** Angle difference (degrees):
\[
\Delta\phi = \bigl| \bigl( (\phi_{\mathrm{rx}} - \phi_{\mathrm{az}} + 180) \bmod 360 \bigr) - 180 \bigr|.
\]

**(F11)** In-beam gain (cosine-squared taper):
\[
G(\Delta\phi) = G_{\max}\, \cos^2\left( \frac{\Delta\phi}{\theta_{\mathrm{bw}}/2} \cdot \frac{\pi}{2} \right),\quad \Delta\phi \le \theta_{\mathrm{bw}}/2.
\]

**(F12)** Out-of-beam gain:
\[
G(\Delta\phi) = 0.01\,G_{\max},\quad \Delta\phi > \theta_{\mathrm{bw}}/2.
\]

### 2.6 Received Power

**(F13)** Received power (dBm):
\[
P_{\mathrm{rx}} = P_{\mathrm{tx}} + G - \mathrm{PL}(d) - A_{\mathrm{weather}}(w).
\]

### 2.7 Interference

**(F14)** Number of interferers (from vehicle density):
\[
N_{\mathrm{int}} = \max\left(0,\; \left\lfloor \frac{\rho_{\mathrm{veh}}}{50} \right\rfloor - 1 \right).
\]

**(F15)** Per-interferer distance factor (uniform):
\[
\alpha_i \sim \mathcal{U}(1.5,\, 3.0).
\]

**(F16)** Path-loss difference for interferer \(i\) (dB):
\[
\Delta\mathrm{PL}_i = 10\,n\,\log_{10}(\alpha_i) + u_i,\quad u_i \sim \mathcal{U}(5,\, 15).
\]

**(F17)** Interferer power (linear, mW):
\[
I_i^{\mathrm{lin}} = 10^{(P_{\mathrm{rx}} - \Delta\mathrm{PL}_i)/10}.
\]

**(F18)** Total interference power (dBm):
\[
I_{\mathrm{tot}}^{\mathrm{dB}} = 10\,\log_{10}\left( \sum_i I_i^{\mathrm{lin}} \right),\quad N_{\mathrm{int}} \ge 1.
\]

**(F19)** If \(N_{\mathrm{int}}=0\): \(I_{\mathrm{tot}}^{\mathrm{dB}} = -\infty\).

### 2.8 SINR

**(F20)** Signal and noise in linear (mW):
\[
S^{\mathrm{lin}} = 10^{P_{\mathrm{rx}}/10},\quad N^{\mathrm{lin}} = 10^{N_0/10}.
\]

**(F21)** Interference in linear:
\[
I^{\mathrm{lin}} = \begin{cases} 10^{I_{\mathrm{tot}}^{\mathrm{dB}}/10} & I_{\mathrm{tot}}^{\mathrm{dB}} > -\infty \\ 0 & \text{otherwise}. \end{cases}
\]

**(F22)** SINR (linear):
\[
\mathrm{SINR}^{\mathrm{lin}} = \frac{S^{\mathrm{lin}}}{I^{\mathrm{lin}} + N^{\mathrm{lin}}}.
\]

**(F23)** SINR (dB):
\[
\gamma = \mathrm{SINR}_{\mathrm{dB}} = 10\,\log_{10}(\mathrm{SINR}^{\mathrm{lin}}).
\]

### 2.9 Coverage Radius (Reverse Path Loss)

**(F24)** Effective minimum RX power (with variation):
\[
P_{\mathrm{min,eff}} = -85 + v,\quad v \sim \mathcal{U}(-5,\, 5).
\]

**(F25)** Interference margin (dB):
\[
M_{\mathrm{int}} = N_{\mathrm{int}} \times 1.5.
\]

**(F26)** Maximum allowable path loss:
\[
\mathrm{PL}_{\max} = P_{\mathrm{tx}} + G_{\mathrm{avg}} - P_{\mathrm{min,eff}} - A_{\mathrm{weather}}(w) - M_{\mathrm{int}}.
\]

**(F27)** Average antenna gain used for radius: \(G_{\mathrm{avg}} = G_{\max} \times 0.7\).

**(F28)** If \(\mathrm{PL}_{\max} \le L_0\), set \(R_{\mathrm{cov}} = 100\) m.

**(F29)** Otherwise, log-distance:
\[
\log_{10}(d) = \frac{\mathrm{PL}_{\max} - L_0}{10\,n_{\mathrm{eff}}} + \log_{10}(d_0).
\]

**(F30)** Distance before shadowing margin:
\[
d' = 10^{\log_{10}(d)}.
\]

**(F31)** Shadowing margin factor:
\[
m_{\mathrm{sh}} = 0.5\,\sigma_{\mathrm{sh}},\quad d'' = d' \cdot 10^{-m_{\mathrm{sh}}/(10\,n_{\mathrm{eff}})}.
\]

**(F32)** Coverage radius (clipped):
\[
R_{\mathrm{cov}} = \mathrm{clip}(d'',\; 200,\; 1000)\;\text{m}.
\]

### 2.10 SINR and Environment Factors for Radius

**(F33)** SINR normalized to [0,1]:
\[
\gamma_{\mathrm{norm}} = \frac{\gamma + 50}{100},\quad \gamma_{\mathrm{norm}} \in [0,1].
\]

**(F34)** SINR factor:
\[
f_{\mathrm{SINR}} = 0.8 + \gamma_{\mathrm{norm}} \times 0.4,\quad f_{\mathrm{SINR}} \in [0.8,\, 1.2].
\]

**(F35)** Environment factor:
\[
f_{\mathrm{env}} = \frac{1}{\sqrt{\eta_{\mathrm{obs}}(k)}}.
\]

**(F36)** Final coverage radius:
\[
R_{\mathrm{cov}}^{\mathrm{final}} = R_{\mathrm{cov}} \cdot f_{\mathrm{SINR}} \cdot f_{\mathrm{env}},\quad R_{\mathrm{cov}}^{\mathrm{final}} = \mathrm{clip}(R_{\mathrm{cov}}^{\mathrm{final}},\; 150,\; 1000).
\]

### 2.11 Coverage Area and Binary Metrics

**(F37)** Coverage area (km²):
\[
A_{\mathrm{cov}} = \pi \left( \frac{R_{\mathrm{cov}}^{\mathrm{final}}}{1000} \right)^2.
\]

**(F38)** Coverage probability (sigmoid in SINR):
\[
P_{\mathrm{cov}} = \frac{1}{1 + \exp(-(\gamma - \gamma_0)/s)},\quad \gamma_0=0,\; s=5.
\]

**(F39)** Is covered: \(\mathbb{1}[\gamma \ge \gamma_{\min}]\).

**(F40)** Is good quality: \(\mathbb{1}[\gamma \ge 10\;\text{dB}]\).

### 2.12 QoS Score

**(F41)** SINR component (0–40):
\[
s_{\mathrm{SINR}} = \mathrm{clip}\left( \frac{\gamma + 10}{30} \times 40,\; 0,\; 40 \right).
\]

**(F42)** Power component (0–20):
\[
s_{\mathrm{power}} = \mathrm{clip}\left( \frac{P_{\mathrm{rx}} + 80}{50} \times 20,\; 0,\; 20 \right).
\]

**(F43)** Distance component (0–20):
\[
s_{\mathrm{dist}} = \mathrm{clip}\left( \frac{1000 - d}{1000} \times 20,\; 0,\; 20 \right).
\]

**(F44)** Coverage component (0–20):
\[
s_{\mathrm{cov}} = P_{\mathrm{cov}} \times 20.
\]

**(F45)** QoS (0–100):
\[
\mathrm{QoS} = \mathrm{clip}(s_{\mathrm{SINR}} + s_{\mathrm{power}} + s_{\mathrm{dist}} + s_{\mathrm{cov}},\; 0,\; 100).
\]

### 2.13 Throughput (Shannon)

**(F46)** SINR in linear: \(\mathrm{SINR}^{\mathrm{lin}} = 10^{\gamma/10}\).

**(F47)** Capacity (Mbps):
\[
C = B \times \log_2(1 + \mathrm{SINR}^{\mathrm{lin}}).
\]

**(F48)** Throughput (efficiency 0.7):
\[
T = \max(0.7 \times C,\; 0.1).
\]

### 2.14 Input Feature Ranges (Sampling)

**(F49)** RSU x, y (m): \(x_{\mathrm{rsu}}, y_{\mathrm{rsu}} \sim \mathcal{U}(0, 2000)\).

**(F50)** TX power (dBm): \(P_{\mathrm{tx}} \sim \mathcal{U}(30, 36)\).

**(F51)** Antenna tilt (deg): \(\theta_{\mathrm{tilt}} \sim \mathcal{U}(0, 15)\).

**(F52)** Antenna azimuth (deg): \(\phi_{\mathrm{az}} \in \{0, 60, 120, 180, 240, 300\}\).

**(F53)** Building density: \(k \sim \mathrm{Uniform}\{0,1,2,3\}\).

**(F54)** Weather: \(w \sim \mathrm{Categorical}([0.6, 0.2, 0.15, 0.05])\).

**(F55)** Vehicle density (per km²): \(\rho_{\mathrm{veh}} = \mathrm{clip}(\mathrm{Lognormal}(\ln 50, 0.6), 5, 200)\).

**(F56)** Distance (m): \(d \sim \mathcal{U}(10, 1000)\).

**(F57)** Angle (deg): \(\phi_{\mathrm{rx}} \sim \mathcal{U}(0, 360)\).

**(F58)** RX height: \(h_{\mathrm{rx}} = 1.5\) m.

**(F59)** Frequency: \(f = 5.9\) GHz.

---

## 3. Part II: MLP Backbone

The **CoverageMLP** is an 8-layer feedforward network with LayerNorm and GELU. Input dimension 13, hidden 256, output 5 (Power, SINR, Radius, Area, QoS).

### 3.1 Notation

- \( \mathbf{x} \in \mathbb{R}^{13}\): input (normalized features).
- \( \mathbf{h}^{(0)} = \mathbf{x} \).
- \( \mathbf{W}^{(l)}, \mathbf{b}^{(l)}\): weight and bias of layer \(l\).
- \( \mathbf{h}^{(l)}\): hidden state after layer \(l\).

### 3.2 Layer 0 (Input → Hidden)

**(F60)** Linear:
\[
\mathbf{z}^{(0)} = \mathbf{W}^{(0)} \mathbf{h}^{(0)} + \mathbf{b}^{(0)},\quad \mathbf{W}^{(0)} \in \mathbb{R}^{256\times 13},\; \mathbf{b}^{(0)} \in \mathbb{R}^{256}.
\]

**(F61)** LayerNorm (per dimension \(j\)):
\[
\mu_j = \frac{1}{H}\sum_{i=1}^{H} z_i^{(0)},\quad \sigma_j^2 = \frac{1}{H}\sum_{i=1}^{H}(z_i^{(0)} - \mu_j)^2 + \epsilon.
\]

**(F62)** Normalized and affine:
\[
\tilde{z}_j = \frac{z_j^{(0)} - \mu_j}{\sqrt{\sigma_j^2}},\quad \hat{z}_j = \gamma_j \tilde{z}_j + \beta_j.
\]

**(F63)** GELU:
\[
\mathrm{GELU}(x) = x\,\Phi(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{2/\pi}(x + 0.044715 x^3)\right)\right).
\]

**(F64)** \(\mathbf{h}^{(1)} = \mathrm{GELU}(\mathrm{LayerNorm}(\mathbf{z}^{(0)}))\).

### 3.3 Layers 1–7 (Hidden → Hidden)

**(F65)** For \(l = 1,\ldots,7\):
\[
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{h}^{(l)} + \mathbf{b}^{(l)},\quad \mathbf{W}^{(l)} \in \mathbb{R}^{256\times 256}.
\]

**(F66)** \(\mathbf{h}^{(l+1)} = \mathrm{Dropout}(\mathrm{GELU}(\mathrm{LayerNorm}(\mathbf{z}^{(l)}))),\quad p=0.1\).

### 3.4 Output Heads

**(F67)** Backbone output: \(\mathbf{h} = \mathbf{h}^{(8)} \in \mathbb{R}^{256}\).

**(F68)** Power head: \(y_{\mathrm{power}} = \mathbf{w}_{\mathrm{power}}^{\top} \mathbf{h} + b_{\mathrm{power}}\).

**(F69)** SINR head: \(y_{\mathrm{sinr}} = \mathbf{w}_{\mathrm{sinr}}^{\top} \mathbf{h} + b_{\mathrm{sinr}}\).

**(F70)** Radius head: \(y_{\mathrm{radius}} = \mathbf{w}_{\mathrm{radius}}^{\top} \mathbf{h} + b_{\mathrm{radius}}\).

**(F71)** Area head: \(y_{\mathrm{area}} = \mathbf{w}_{\mathrm{area}}^{\top} \mathbf{h} + b_{\mathrm{area}}\).

**(F72)** QoS head: \(y_{\mathrm{qos}} = \mathbf{w}_{\mathrm{qos}}^{\top} \mathbf{h} + b_{\mathrm{qos}}\).

**(F73)** Output vector: \(\mathbf{y} = [y_{\mathrm{power}},\, y_{\mathrm{sinr}},\, y_{\mathrm{radius}},\, y_{\mathrm{area}},\, y_{\mathrm{qos}}]^{\top}\).

### 3.5 Weight Initialization

**(F74)** Xavier uniform for Linear (gain \(g=0.5\)):
\[
W_{ij} \sim \mathcal{U}\left( -\frac{g\sqrt{6}}{\sqrt{n_{\mathrm{in}}+n_{\mathrm{out}}}},\; \frac{g\sqrt{6}}{\sqrt{n_{\mathrm{in}}+n_{\mathrm{out}}}} \right).
\]

**(F75)** Bias: \(b_i = 0\).

**(F76)** LayerNorm: \(\gamma = 1,\; \beta = 0\) (standard).

### 3.6 Parameter Count (Approximate)

**(F77)** Layer 0: \(13\times 256 + 256 = 3584\); LayerNorm: \(256\times 2 = 512\).  
**(F78)** Layers 1–7: each \(256\times 256 + 256 = 65792\), plus LayerNorm 512, total per layer 66304; 7 layers: 464128.  
**(F79)** Heads: \(5 \times (256 \times 1 + 1) = 1285\).  
**(F80)** Total ≈ 469K parameters.

---

## 4. Part III: Mamba-3 Backbone

The **CoverageMamba3** uses an input projection, 8 MambaBlocks, and 5 task heads. Each MambaBlock contains a **SelectiveSSM** with convolution, delta/B/C projection, stable discretization, and skip connection.

### 4.1 Input Projection

**(F81)** Linear: \(\mathbf{u} = \mathbf{W}_{\mathrm{in}} \mathbf{x} + \mathbf{b}_{\mathrm{in}},\quad \mathbf{W}_{\mathrm{in}} \in \mathbb{R}^{256\times 13}\).

**(F82)** Then LayerNorm, GELU, Dropout(0.1). Sequence shape: \((B, 1, 256)\) for single-step.

### 4.2 MambaBlock (Single Block)

**(F83)** Residual: \(\mathbf{x}_{\mathrm{res}} = \mathbf{x}\).

**(F84)** LayerNorm: \(\mathbf{x}_{\mathrm{norm}} = \mathrm{LayerNorm}(\mathbf{x})\).

**(F85)** Input projection (expand ×2): \(\mathbf{x}_{\mathrm{proj}} = \mathbf{W}_{\mathrm{in\_proj}} \mathbf{x}_{\mathrm{norm}},\quad \mathbf{W}_{\mathrm{in\_proj}} \in \mathbb{R}^{(d_{\mathrm{inner}}\times 2) \times d_{\mathrm{model}}} = \mathbb{R}^{1024 \times 256}\).

**(F86)** Split: \(\mathbf{x}_{\mathrm{ssm}}, \mathbf{g} = \mathrm{split}(\mathbf{x}_{\mathrm{proj}},\, 2),\quad \mathbf{x}_{\mathrm{ssm}}, \mathbf{g} \in \mathbb{R}^{d_{\mathrm{inner}}} = \mathbb{R}^{512}\) (per half).

**(F87)** SSM output: \(\mathbf{y}_{\mathrm{ssm}} = \mathrm{SelectiveSSM}(\mathbf{x}_{\mathrm{ssm}})\).

**(F88)** Gate: \(\mathbf{y}_{\mathrm{gate}} = \mathbf{y}_{\mathrm{ssm}} \odot \mathrm{SiLU}(\mathbf{g})\).

**(F89)** Output projection: \(\mathbf{y}_{\mathrm{out}} = \mathbf{W}_{\mathrm{out}} \mathbf{y}_{\mathrm{gate}},\quad \mathbf{W}_{\mathrm{out}} \in \mathbb{R}^{256 \times d_{\mathrm{inner}}} = \mathbb{R}^{256 \times 512}\) (d_inner → d_model).

**(F90)** Residual add: \(\mathbf{x}_{\mathrm{next}} = \mathbf{x}_{\mathrm{res}} + \mathbf{y}_{\mathrm{out}}\).

### 4.3 SelectiveSSM — Convolution

**(F91)** Conv1d: kernel size \(K=4\), padding \(K-1=3\), groups = \(d_{\mathrm{model}}\) (depthwise).
\[
\mathbf{x}_{\mathrm{conv}} = \mathrm{Conv1d}(\mathbf{x}),\quad \mathbf{x}_{\mathrm{conv}} \in \mathbb{R}^{B \times L \times D}.
\]

**(F92)** Truncate to length \(L\): \(\mathbf{x}_{\mathrm{conv}} = \mathbf{x}_{\mathrm{conv}}_{:,:,:L}\).

**(F93)** SiLU: \(\mathbf{x}_{\mathrm{conv}} = \mathrm{SiLU}(\mathbf{x}_{\mathrm{conv}}),\quad \mathrm{SiLU}(x) = x\,\sigma(x)\).

### 4.4 SelectiveSSM — Projections

**(F94)** \([\Delta,\, \mathbf{B},\, \mathbf{C}] = \mathrm{split}(\mathbf{W}_{\mathrm{x\_proj}} \mathbf{x}_{\mathrm{conv}},\, [D,\, D,\, D])\).

**(F95)** Delta (time step): \(\Delta = \mathrm{softplus}(\mathbf{W}_{\mathrm{dt}} \Delta + b_{\mathrm{dt}})\).

**(F96)** Clamp for stability: \(\Delta = \mathrm{clip}(\Delta,\; 0.001,\; 1.0)\).

**(F97)** State matrix (continuous): \(\mathbf{A} = -\exp(\mathrm{clip}(\mathbf{A}_{\log},\; -10,\; 3))\).

### 4.5 SelectiveSSM — Stable SSM (Simplified)

**(F98)** Weights: \(\mathbf{w} = \sigma(\Delta)\).

**(F99)** Gated input: \(\mathbf{u} = \mathbf{x}_{\mathrm{conv}} \odot \sigma(\mathbf{B}) \odot \mathbf{w}\).

**(F100)** Output (simplified): \(\mathbf{y}_{\mathrm{ssm}} = \mathbf{u} \odot \tanh(\mathbf{C})\).

**(F101)** Skip: \(\mathbf{y}_{\mathrm{ssm}} = \mathbf{y}_{\mathrm{ssm}} + \mathbf{x}_{\mathrm{conv}} \odot \mathbf{D}\).

**(F102)** Output projection: \(\mathbf{y} = \mathbf{W}_{\mathrm{out}} \mathbf{y}_{\mathrm{ssm}}\).

### 4.6 Full Forward (Mamba-3)

**(F103)** Embedding: \(\mathbf{x}^{(0)} = \mathrm{InputProj}(\mathbf{x}_{\mathrm{in}})\).

**(F104)** For \(l = 1,\ldots,8\): \(\mathbf{x}^{(l)} = \mathrm{MambaBlock}(\mathbf{x}^{(l-1)})\).

**(F105)** Final norm: \(\mathbf{x}^{(9)} = \mathrm{LayerNorm}(\mathbf{x}^{(8)})\).

**(F106)** Pool: \(\bar{\mathbf{x}} = \mathrm{mean}(\mathbf{x}^{(9)},\, \mathrm{dim}=\mathrm{seq})\) or squeeze if seq_len=1.

**(F107)** Heads: same as MLP, \(\mathbf{y} = [y_{\mathrm{power}},\, y_{\mathrm{sinr}},\, y_{\mathrm{radius}},\, y_{\mathrm{area}},\, y_{\mathrm{qos}}]^{\top}\).

### 4.7 Hyperparameters

**(F108)** \(d_{\mathrm{model}} = 256,\; n_{\mathrm{layers}} = 8,\; d_{\mathrm{state}} = 16,\; d_{\mathrm{conv}} = 4,\; \mathrm{expand} = 2,\; d_{\mathrm{inner}} = 512\).

**(F109)** Parameter count ≈ 13.7M.

---

## 5. Part IV: Selector Brain

The Selector runs **PhysicsAnalyzer.analyze** and **PhysicsAnalyzer.check_divergence**, then chooses MLP or Mamba.

### 5.1 Input Symbols

- \(d\): distance_to_rx_m (m)  
- \(w\): weather_condition \(\in \{0,1,2,3\}\)  
- \(k\): building_density \(\in \{0,1,2,3\}\)  
- \(n_{\mathrm{int}}\): num_interferers  

### 5.2 Constants

**(F110)** \(d_{\mathrm{th}} = 500\) m, \(d_{\mathrm{th2}} = 700\) m, \(w_{\mathrm{th}} = 2\), \(k_{\mathrm{th}} = 2\), \(n_{\mathrm{int,th}} = 3\), \(\Delta_{\mathrm{div}} = 5\) dB.

### 5.3 Trigger Score (Cumulative)

**(F111)** Initial: \(s = 0\).

**(F112)** Very long range: if \(d > 700\), \(s \mathrel{+}= 0.35\).

**(F113)** Long range: else if \(d > 500\), \(s \mathrel{+}= 0.20\).

**(F114)** Distance + rain: if \(d > 500\) and \(w \ge 2\), \(s \mathrel{+}= 0.20\).

**(F115)** Heavy rain only: else if \(w \ge 3\), \(s \mathrel{+}= 0.10\).

**(F116)** Distance + urban: if \(d > 500\) and \(k \ge 2\), \(s \mathrel{+}= 0.15\).

**(F117)** Range + interference: if \(n_{\mathrm{int}} \ge 3\) and \(d > 400\), \(s \mathrel{+}= 0.10\).

**(F118)** Risk factor count: \(n_{\mathrm{risk}} = \mathbb{1}[w\ge 2] + \mathbb{1}[d>500] + \mathbb{1}[k\ge 2] + \mathbb{1}[n_{\mathrm{int}}\ge 3]\).

**(F119)** Triple compound: if \(n_{\mathrm{risk}} \ge 3\), \(s \mathrel{+}= 0.15\).

**(F120)** Clamp: \(s = \min(s,\, 1.0)\).

**(F121)** use_mamba: \(\mathrm{use\_mamba} = (s \ge 0.3)\).

### 5.4 Divergence

**(F122)** Power difference: \(\Delta P = |P_{\mathrm{mlp}} - P_{\mathrm{mamba}}|\).

**(F123)** SINR difference: \(\Delta\gamma = |\gamma_{\mathrm{mlp}} - \gamma_{\mathrm{mamba}}|\).

**(F124)** Radius difference: \(\Delta R = |R_{\mathrm{mlp}} - R_{\mathrm{mamba}}|\).

**(F125)** Divergent: \(\mathrm{divergent} = (\Delta P > \Delta_{\mathrm{div}}) \lor (\Delta\gamma > \Delta_{\mathrm{div}})\).

### 5.5 Decision Rule

**(F126)** Final Mamba flag: \(\mathrm{use\_mamba}^{\mathrm{final}} = \mathrm{use\_mamba} \lor \bigl( \mathrm{divergent} \land (s \ge 0.15) \bigr)\).

**(F127)** If \(\mathrm{use\_mamba}^{\mathrm{final}}\): return Mamba prediction; else return MLP prediction.

### 5.6 Risk Level

**(F128)** risk_level = "EXTREME" if \(s \ge 0.5\), else "ELEVATED" if \(s \ge 0.3\), else "NORMAL".

### 5.7 Batch Prediction

**(F129)** For each sample in batch: run predict; collect results.  
**(F130)** mamba_activations = count of samples where selected_model contains "Mamba".  
**(F131)** mlp_decisions = batch_size - mamba_activations.

---

## 6. Part V: Training

Both MLP and Mamba use the same feature normalization, target scaling, and multi-task loss.

### 6.1 Feature Normalization

**(F132)** Mean: \(\boldsymbol{\mu} = \frac{1}{N}\sum_{n=1}^{N} \mathbf{x}_n\).

**(F133)** Std: \(\boldsymbol{\sigma} = \sqrt{\frac{1}{N}\sum_{n}(\mathbf{x}_n - \boldsymbol{\mu})^2 + \epsilon},\quad \epsilon=10^{-8}\).

**(F134)** Normalized: \(\tilde{\mathbf{x}}_n = (\mathbf{x}_n - \boldsymbol{\mu}) / \boldsymbol{\sigma}\).

### 6.2 Target Scaling (Same for Both Models)

**(F135)** Power: \(t_0' = (t_0 + 260) / 230,\quad t_0 \in [-260, -30]\) → \([0, 1]\).

**(F136)** SINR: \(t_1' = (t_1 + 170) / 230\).

**(F137)** Radius: \(t_2' = (t_2 - 150) / 90\).

**(F138)** Area: \(t_3' = (t_3 - 0.07) / 0.12\).

**(F139)** QoS: \(t_4' = t_4 / 100\).

**(F140)** Clip: \(\mathbf{t}' = \mathrm{clip}(\mathbf{t}',\; 0,\; 1)\).

### 6.3 Multi-Task Loss (MLP and Mamba)

**(F141)** Per-output MSE: \(\mathcal{L}_i = \frac{1}{B}\sum_{b}(y_{b,i} - t_{b,i}')^2,\quad i \in \{0,1,2,3,4\}\).

**(F142)** Weighted sum:
\[
\mathcal{L} = 0.15\,\mathcal{L}_0 + 0.15\,\mathcal{L}_1 + 0.30\,\mathcal{L}_2 + 0.30\,\mathcal{L}_3 + 0.10\,\mathcal{L}_4.
\]

**(F143)** Penalty (radius ≥ 0): \(\mathcal{P}_2 = 0.05 \sum_b \mathrm{ReLU}(-y_{b,2})\).

**(F144)** Penalty (area ≥ 0): \(\mathcal{P}_3 = 0.05 \sum_b \mathrm{ReLU}(-y_{b,3})\).

**(F145)** Penalty (QoS ≥ 0): \(\mathcal{P}_{4a} = 0.02 \sum_b \mathrm{ReLU}(-y_{b,4})\).

**(F146)** Penalty (QoS ≤ 1): \(\mathcal{P}_{4b} = 0.02 \sum_b \mathrm{ReLU}(y_{b,4} - 1)\).

**(F147)** Total loss: \(\mathcal{L}_{\mathrm{total}} = \mathcal{L} + \mathcal{P}_2 + \mathcal{P}_3 + \mathcal{P}_{4a} + \mathcal{P}_{4b}\).

### 6.4 Optimizer and Scheduler (MLP)

**(F148)** AdamW: \(\theta \leftarrow \theta - \eta\,(\nabla_{\theta}\mathcal{L} + \lambda \theta),\quad \eta=10^{-3},\; \lambda=0.01\).

**(F149)** Cosine annealing: \(\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t/T)),\quad T=150,\; \eta_{\min}=10^{-5}\).

### 6.5 Learning Rate Schedule (Mamba) — 3-Stage

**(F150)** Warmup (epochs 0–9): \(\eta_t = \eta_{\mathrm{base}} \cdot (t+1) / 10,\quad \eta_{\mathrm{base}}=10^{-4}\).

**(F151)** Stable (epochs 10–99): \(\eta_t = \eta_{\mathrm{base}}\).

**(F152)** Decay (epochs 100–149): \(\tau = (t - 100) / 50\), \(\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\mathrm{base}} - \eta_{\min})(1 + \cos(\pi \tau)),\quad \eta_{\min}=10^{-6}\).

### 6.6 Gradient Clipping

**(F153)** \(\mathbf{g} \leftarrow \mathbf{g} \cdot \min(1,\; \theta_{\max} / \|\mathbf{g}\|),\quad \theta_{\max}=1\).

### 6.7 Metrics — R² (Per Target)

**(F154)** SS_res: \(\mathrm{SS}_{\mathrm{res},i} = \sum_b (t_{b,i}' - y_{b,i})^2\).

**(F155)** SS_tot: \(\mathrm{SS}_{\mathrm{tot},i} = \sum_b (t_{b,i}' - \bar{t}_i')^2\).

**(F156)** R²: \(R^2_i = 1 - \mathrm{SS}_{\mathrm{res},i} / (\mathrm{SS}_{\mathrm{tot},i} + \epsilon)\).

**(F157)** R² overall: \(R^2_{\mathrm{overall}} = \frac{1}{5}\sum_{i=0}^{4} R^2_i\).

### 6.8 Metrics — MAE (Per Target)

**(F158)** MAE: \(\mathrm{MAE}_i = \frac{1}{B}\sum_b |t_{b,i}' - y_{b,i}|\).

**(F159)** MAE overall: \(\mathrm{MAE}_{\mathrm{overall}} = \frac{1}{5}\sum_i \mathrm{MAE}_i\).

### 6.9 Dataset Split

**(F160)** Train: 21000, Val: 4500, Test: 4500 (Mamba) or remaining (MLP).  
**(F161)** Seed: 42 for reproducibility.

---

## 7. Part VI: Experiment Design & Plan

Seven experiments validate distribution, physics consistency, rainstorm, stratification, cost, ablation, and tail risk.

### 7.1 Exp1: Scenario Distribution (No API)

**Objective:** Describe the training data w.r.t. extreme vs normal and marginals.

**(F162)** Total samples: \(N = |\mathcal{D}|\).

**(F163)** Extreme definition: sample \(n\) is extreme if any of: \(w_n \ge 2\), \(d_n > 500\), \(k_n \ge 2\), \(n_{\mathrm{int},n} \ge 3\).

**(F164)** Extreme count: \(N_{\mathrm{ext}} = \sum_n \mathbb{1}[\text{sample } n \text{ extreme}]\).

**(F165)** Normal count: \(N_{\mathrm{norm}} = N - N_{\mathrm{ext}}\).

**(F166)** Extreme percentage: \(\mathrm{ext}\% = 100 \cdot N_{\mathrm{ext}} / N\).

**(F167)** Weather marginal: for each \(w\), count \(c_w = \sum_n \mathbb{1}[w_n = w]\).

**(F168)** Density marginal: for each \(k\), count \(c_k = \sum_n \mathbb{1}[k_n = k]\).

**(F169)** Distance buckets: \(b_1 = [0,200),\; b_2 = [200,500),\; b_3 = [500,800),\; b_4 = [800,\infty)\); counts per bucket.

**(F170)** Type counts: heavy_weather, long_distance, dense_urban, high_interference (each defined by one condition); extreme = union.

### 7.2 Exp2: Distance–Power Decay (API or Local)

**Objective:** Compare slope of power vs \(\log_{10}(d)\) for MLP, Mamba, Dual vs theory.

**(F171)** Distances: \(d_i \in [50, 1000]\) m, 60 points (e.g. linspace).

**(F172)** For each environment (e.g. Rural \(k=0\), Urban \(k=2\)): theory slope (dB per decade):
\[
s_{\mathrm{theory}} = -10 \cdot n_{\mathrm{eff}} = -10 \cdot n \cdot \eta_{\mathrm{obs}}(k).
\]

**(F173)** For each \(d_i\): get MLP, Mamba, Dual (Selector) power; store \(P_{\mathrm{mlp}}(d_i), P_{\mathrm{mamba}}(d_i), P_{\mathrm{dual}}(d_i)\).

**(F174)** Linear fit in log-distance: \(\log_{10}(d_i/d_0)\), fit \(P \approx a \log_{10}(d/d_0) + b\).

**(F175)** Slope error: \(\Delta s = a - s_{\mathrm{theory}}\).

**(F176)** Report: Rural/Urban × (mlp_slope, mamba_slope, dual_slope, theory_slope, slope_error_dB for each).

### 7.3 Exp3: Rainstorm Coverage (API or Local)

**Objective:** Power vs weather at fixed distance (e.g. 300 m) and two densities.

**(F177)** Fixed \(d = 300\) m; densities Rural (0), Suburban (1); weather \(w \in \{0,1,2,3\}\).

**(F178)** For each (density, weather): get physics (formula), MLP, Mamba, Dual power; trigger_score.

**(F179)** Physics power: \(P_{\mathrm{physics}} = P_{\mathrm{tx}} + G_{\mathrm{avg}} - \mathrm{PL}(d) - A_{\mathrm{weather}}(w)\).

**(F180)** theory_atten = \(A_{\mathrm{weather}}(w)\).

### 7.4 Exp4: Stratified Performance (API or Local)

**Objective:** MSE by category (normal, extreme_weather, extreme_distance, extreme_density, extreme_compound).

**(F181)** Test set: last 4500 samples (or held-out).

**(F182)** Category indices:  
- normal: \(w<2,\, d\le 500,\, k<2,\, n_{\mathrm{int}}<3\).  
- extreme_weather: \(w\ge 2\).  
- extreme_distance: \(d>500\).  
- extreme_density: \(k\ge 2\).  
- extreme_compound: at least 2 of the above.

**(F183)** Per-sample error (MSE over 5 outputs): \(\mathrm{err}(\hat{\mathbf{y}}, \mathbf{t}) = \frac{1}{5}\sum_{i=0}^{4}(\hat{y}_i - t_i)^2\).

**(F184)** Per category \(\mathcal{C}\): \(\mathrm{MSE}_{\mathrm{mlp}}(\mathcal{C}) = \frac{1}{|\mathcal{C}|}\sum_{i \in \mathcal{C}} \mathrm{err}(\hat{y}_{\mathrm{mlp},i}, \mathbf{t}_i)\).

**(F185)** Similarly \(\mathrm{MSE}_{\mathrm{mamba}}(\mathcal{C})\), \(\mathrm{MSE}_{\mathrm{dual}}(\mathcal{C})\).

**(F186)** MLP improvement: \(\mathrm{imp}(\mathcal{C}) = 100 \cdot (\mathrm{MSE}_{\mathrm{mlp}}(\mathcal{C}) - \mathrm{MSE}_{\mathrm{dual}}(\mathcal{C})) / \mathrm{MSE}_{\mathrm{mlp}}(\mathcal{C})\) if \(\mathrm{MSE}_{\mathrm{mlp}}>0\).

### 7.5 Exp5: Cost / Latency (API)

**Objective:** Latency and parameter count for MLP, Mamba, D²TL (parallel and early-exit).

**(F187)** Per service: 50 × POST /predict; latency_ms = mean(time per request in ms).

**(F188)** Params: from /health (parameters field).

**(F189)** D²TL parallel: latency = max(MLP_latency, Mamba_latency) (both run).

**(F190)** Trigger rate (e.g. 0.15): fraction of requests that use Mamba.

**(F191)** D²TL early-exit latency: \(T_{\mathrm{ee}} = r\, T_{\mathrm{parallel}} + (1-r)\, T_{\mathrm{mlp}},\quad r=0.15\).

**(F192)** Speedup vs Mamba only: \(\mathrm{speedup} = T_{\mathrm{mamba}} / T_{\mathrm{ee}}\).

### 7.6 Exp6: Ablation (API or Local)

**Objective:** Compare 6 strategies on same test set: MLP Only, Mamba Only, Random 50/50, Any-Extreme Trigger, D²TL Selector, Soft Blend 0.5.

**(F193)** Same test set and same batch responses (each sample has mlp_prediction, mamba_prediction, and final selected output).

**(F194)** MLP Only: \(\mathrm{MSE}_{\mathrm{mlp}} = \frac{1}{N}\sum_n \mathrm{err}(\hat{y}_{\mathrm{mlp},n}, \mathbf{t}_n)\).

**(F195)** Mamba Only: \(\mathrm{MSE}_{\mathrm{mamba}} = \frac{1}{N}\sum_n \mathrm{err}(\hat{y}_{\mathrm{mamba},n}, \mathbf{t}_n)\).

**(F196)** Random 50/50: for each \(n\), pick MLP or Mamba with probability 0.5 (fixed seed); \(\mathrm{MSE}_{\mathrm{rand}}\).

**(F197)** Any-Extreme Trigger: if sample \(n\) is extreme (any of \(w\ge 2, d>500, k\ge 2, n_{\mathrm{int}}\ge 3\)), use Mamba; else MLP. \(\mathrm{MSE}_{\mathrm{any}}\).

**(F198)** D²TL Selector: use actual Selector output. \(\mathrm{MSE}_{\mathrm{d2tl}}\).

**(F199)** Soft Blend: \(\hat{y}_{\mathrm{blend},n} = 0.5\,\hat{y}_{\mathrm{mlp},n} + 0.5\,\hat{y}_{\mathrm{mamba},n}\). \(\mathrm{MSE}_{\mathrm{blend}}\).

### 7.7 Exp7: Tail Risk (API or Local)

**Objective:** Error distribution (mean, median, p90, p95, p99, max) for MLP, Mamba, D²TL; tail improvement vs MLP.

**(F200)** Per-sample errors: \(\mathrm{err}_n^{\mathrm{mlp}}, \mathrm{err}_n^{\mathrm{mamba}}, \mathrm{err}_n^{\mathrm{dual}}\).

**(F201)** Mean: \(\bar{e} = \frac{1}{N}\sum_n e_n\).

**(F202)** Median: \(\tilde{e} = \mathrm{median}(\{e_n\})\).

**(F203)** Percentile: \(e_{p} = \mathrm{percentile}(\{e_n\},\; p)\).

**(F204)** Tail improvement (e.g. p95): \(\mathrm{imp}_{95} = 100 \cdot \left(1 - \frac{e_{95}^{\mathrm{dual}}}{e_{95}^{\mathrm{mlp}}}\right)\) if \(e_{95}^{\mathrm{mlp}}>0\).

**(F205)** Similarly \(\mathrm{imp}_{99}\), \(\mathrm{imp}_{\max}\).

### 7.8 Batch API (Exp4, Exp6, Exp7)

**(F206)** Test size \(N_{\mathrm{test}} = 4500\); batch size \(B_{\mathrm{batch}} = 80\); number of batches \(K = \lceil N_{\mathrm{test}} / B_{\mathrm{batch}} \rceil\).

**(F207)** For each batch \(k\): POST /predict/batch with list of \(B_{\mathrm{batch}}\) inputs; receive list of predictions in same order.

**(F208)** Concatenate all predictions to get one vector of length \(N_{\mathrm{test}}\) for MLP, Mamba, and Dual (selected).

### 7.9 Physics Constants in Experiments

**(F209)** CARRIER_FREQ_GHZ = 5.9, TX_POWER_DBM = 33, PATH_LOSS_EXPONENT = 3.5, REFERENCE_DISTANCE_M = 1.0.  
**(F210)** WAVELENGTH_M = c / (f × 10^9), L0_DB = 20×log10(4π×d0/λ).  
**(F211)** DENSITY_OBSTACLE = {0:1, 1:1.2, 2:1.5, 3:2}, WEATHER_ATTEN = {0:0, 1:2, 2:5, 3:8}.  
**(F212)** ANTENNA_GAIN (average) = 10×0.7 dBi.

---

## 8. Part VII: Summary Tables

### 8.1 Formula Index (Selected)

| Range | Topic |
|-------|--------|
| F1–F23 | Physics: wavelength, path loss, antenna, interference, SINR |
| F24–F48 | Data gen: coverage radius, area, QoS, throughput, sampling |
| F49–F59 | Input sampling ranges |
| F60–F80 | MLP: layers, LayerNorm, GELU, heads, init, param count |
| F81–F109 | Mamba-3: blocks, SelectiveSSM, conv, delta/B/C, stable SSM, hyperparams |
| F110–F131 | Selector: constants, trigger score, divergence, decision, batch |
| F132–F161 | Training: normalization, scaling, loss, optimizer, LR, metrics, split |
| F162–F212 | Experiments: Exp1–Exp7 and batch API |

### 8.2 Model Summary

| Model | Input | Output | Params | Latency (typical) |
|-------|--------|--------|--------|--------------------|
| CoverageMLP | 13 (norm) | 5 (scaled) | ~469K | ~1–7 ms |
| CoverageMamba3 | 13 (norm) | 5 (scaled) | ~13.7M | ~60–80 ms |
| Selector | 13 (raw) | MLP or Mamba output | 0 | + network RTT |

### 8.3 Experiment Summary

| Exp | Name | Data source | Main metrics |
|-----|------|-------------|--------------|
| 1 | Distribution | Training data | total, extreme%, weather/density/distance marginals |
| 2 | Distance–Power | API/local sweep | slopes, slope_error_dB vs theory |
| 3 | Rainstorm | API/local 4×2 | physics, mlp, mamba, dual, trigger per (weather, density) |
| 4 | Stratified | API/local batch | n, mlp_mse, mamba_mse, dual_mse, mlp_improvement per category |
| 5 | Cost | API timing | latency_ms, params, speedup |
| 6 | Ablation | API/local batch | MSE for 6 strategies |
| 7 | Tail risk | API/local batch | mean, median, p90, p95, p99, max; tail_improvement |

### 8.4 File Reference

- **Data generator:** `data_generator/generate_coverage_data_v2.py`  
- **MLP:** `d2tl/mlp_service/model.py`  
- **Mamba-3:** `models/mamba3_coverage.py`  
- **Selector:** `d2tl/selector_brain/selector.py`  
- **MLP training:** `d2tl/training/train_mlp.py`  
- **Mamba training:** `training/train_coverage.py`  
- **Experiments (local):** `d2tl/experiments/run_all_experiments.py`  
- **Experiments (via API):** `d2tl/experiments/run_all_experiments_via_api.py`  
- **Backbone docs:** `paper_package/08_backbones/BACKBONE_MLP.md`, `BACKBONE_MAMBA3.md`

---

## 9. Appendix A: Complete Formula List (F1–F212)

For quick lookup, every formula is listed below with a one-line description.

| ID | One-line description |
|----|------------------------|
| F1 | Wavelength λ = c/f_c |
| F2 | Free-space path loss at d0: L0 = 20 log10(4π d0/λ) |
| F3 | Numerical L0 ≈ 47.86 dB |
| F4 | Obstacle factor η_obs(k) for density k |
| F5 | Effective path-loss exponent n_eff = n·η_obs(k) |
| F6 | Weather attenuation A_weather(w) for weather w |
| F7 | Log-distance path loss PL_log(d) |
| F8 | Shadowing X_σ ~ N(0, σ_sh²) |
| F9 | Total path loss PL(d) with floor 40 dB |
| F10 | Angle difference Δφ for antenna gain |
| F11 | In-beam gain G(Δφ) cosine-squared |
| F12 | Out-of-beam gain 0.01·G_max |
| F13 | Received power P_rx = P_tx + G - PL - A_weather |
| F14 | Number of interferers from vehicle density |
| F15 | Per-interferer distance factor α_i ~ U(1.5, 3) |
| F16 | Path-loss difference for interferer i |
| F17 | Interferer power linear I_i^lin |
| F18 | Total interference power dB |
| F19 | Zero interferers → I = -∞ |
| F20 | Signal and noise in linear |
| F21 | Interference linear from dB |
| F22 | SINR linear = S/(I+N) |
| F23 | SINR dB = 10 log10(SINR_lin) |
| F24 | Effective min RX power with variation |
| F25 | Interference margin M_int |
| F26 | Max allowable path loss PL_max |
| F27 | Average antenna gain G_avg = 0.7·G_max |
| F28 | If PL_max ≤ L0 then R_cov = 100 m |
| F29 | Log-distance for coverage radius |
| F30 | Distance d' before shadowing margin |
| F31 | Shadowing margin factor and d'' |
| F32 | Coverage radius clip(200, 1000) |
| F33 | SINR normalized to [0,1] |
| F34 | SINR factor f_SINR ∈ [0.8, 1.2] |
| F35 | Environment factor f_env = 1/√η_obs |
| F36 | Final R_cov with factors, clip(150, 1000) |
| F37 | Coverage area A_cov = π(R/1000)² |
| F38 | Coverage probability sigmoid in SINR |
| F39 | Is covered indicator |
| F40 | Is good quality indicator |
| F41 | QoS SINR component 0–40 |
| F42 | QoS power component 0–20 |
| F43 | QoS distance component 0–20 |
| F44 | QoS coverage component 0–20 |
| F45 | QoS total clip(0, 100) |
| F46 | SINR linear from dB |
| F47 | Shannon capacity C = B log2(1+SINR_lin) |
| F48 | Throughput T = max(0.7·C, 0.1) |
| F49 | RSU x,y ~ U(0, 2000) |
| F50 | TX power ~ U(30, 36) |
| F51 | Tilt ~ U(0, 15) |
| F52 | Azimuth ∈ {0,60,120,180,240,300} |
| F53 | Building density uniform {0,1,2,3} |
| F54 | Weather categorical [0.6,0.2,0.15,0.05] |
| F55 | Vehicle density lognormal |
| F56 | Distance ~ U(10, 1000) |
| F57 | Angle ~ U(0, 360) |
| F58 | RX height 1.5 m |
| F59 | Frequency 5.9 GHz |
| F60 | MLP layer 0 linear z^(0) = W^(0)h^(0)+b^(0) |
| F61 | LayerNorm mean and variance |
| F62 | LayerNorm normalized and affine |
| F63 | GELU activation |
| F64 | h^(1) = GELU(LN(z^(0))) |
| F65 | MLP layers 1–7 linear z^(l) = W^(l)h^(l)+b^(l) |
| F66 | h^(l+1) = Dropout(GELU(LN(z^(l)))) |
| F67 | Backbone output h = h^(8) |
| F68 | Power head |
| F69 | SINR head |
| F70 | Radius head |
| F71 | Area head |
| F72 | QoS head |
| F73 | Output vector y = [power, sinr, radius, area, qos] |
| F74 | Xavier init gain 0.5 |
| F75 | Bias init zero |
| F76 | LayerNorm γ=1, β=0 |
| F77 | MLP layer 0 param count |
| F78 | MLP layers 1–7 param count |
| F79 | MLP heads param count |
| F80 | MLP total ~469K |
| F81 | Mamba input projection linear |
| F82 | Input LayerNorm, GELU, Dropout |
| F83 | MambaBlock residual |
| F84 | MambaBlock LayerNorm |
| F85 | MambaBlock in_proj expand×2 |
| F86 | Split to x_ssm and gate |
| F87 | SelectiveSSM output |
| F88 | Gate y_ssm ⊙ SiLU(g) |
| F89 | MambaBlock out_proj |
| F90 | MambaBlock residual add |
| F91 | SelectiveSSM Conv1d |
| F92 | Conv truncate to L |
| F93 | SiLU after conv |
| F94 | Projection to Δ, B, C |
| F95 | Delta softplus and dt_proj |
| F96 | Delta clamp (0.001, 1) |
| F97 | A = -exp(clamp(A_log)) |
| F98 | Weights w = σ(Δ) |
| F99 | Gated input u = x⊙σ(B)⊙w |
| F100 | Stable SSM output u⊙tanh(C) |
| F101 | Skip y + x⊙D |
| F102 | SSM output projection |
| F103 | Mamba embed x^(0) |
| F104 | Mamba blocks x^(l) = MambaBlock(x^(l-1)) |
| F105 | Final LayerNorm |
| F106 | Pool mean or squeeze |
| F107 | Mamba heads same as MLP |
| F108 | Mamba hyperparameters |
| F109 | Mamba ~13.7M params |
| F110 | Selector constants (d_th, w_th, etc.) |
| F111 | Trigger initial s=0 |
| F112 | Very long range d>700 → +0.35 |
| F113 | Long range d>500 → +0.20 |
| F114 | Distance+rain → +0.20 |
| F115 | Heavy rain w≥3 → +0.10 |
| F116 | Distance+urban → +0.15 |
| F117 | Range+interference → +0.10 |
| F118 | Risk factor count n_risk |
| F119 | Triple compound n_risk≥3 → +0.15 |
| F120 | Score clamp min(s, 1) |
| F121 | use_mamba = (s ≥ 0.3) |
| F122 | Divergence ΔP = |P_mlp - P_mamba| |
| F123 | Divergence Δγ |
| F124 | Divergence ΔR |
| F125 | divergent = (ΔP>5 or Δγ>5) |
| F126 | use_mamba_final = use_mamba or (divergent and s≥0.15) |
| F127 | Return Mamba or MLP |
| F128 | risk_level from score |
| F129 | Batch predict loop |
| F130 | mamba_activations count |
| F131 | mlp_decisions count |
| F132 | Feature mean μ |
| F133 | Feature std σ |
| F134 | Normalized x̃ = (x-μ)/σ |
| F135 | Target scaling power |
| F136 | Target scaling SINR |
| F137 | Target scaling radius |
| F138 | Target scaling area |
| F139 | Target scaling QoS |
| F140 | Target clip [0,1] |
| F141 | Per-output MSE L_i |
| F142 | Weighted loss 0.15+0.15+0.30+0.30+0.10 |
| F143 | Penalty radius ≥ 0 |
| F144 | Penalty area ≥ 0 |
| F145 | Penalty QoS ≥ 0 |
| F146 | Penalty QoS ≤ 1 |
| F147 | Total loss L_total |
| F148 | AdamW update |
| F149 | Cosine annealing LR |
| F150 | Mamba warmup LR |
| F151 | Mamba stable LR |
| F152 | Mamba decay LR |
| F153 | Gradient clip max norm 1 |
| F154 | R² SS_res |
| F155 | R² SS_tot |
| F156 | R²_i = 1 - SS_res/(SS_tot+ε) |
| F157 | R² overall mean |
| F158 | MAE per target |
| F159 | MAE overall |
| F160 | Train/val/test split sizes |
| F161 | Random seed 42 |
| F162 | Exp1 total N |
| F163 | Exp1 extreme definition |
| F164 | Exp1 N_ext |
| F165 | Exp1 N_norm |
| F166 | Exp1 ext% |
| F167 | Exp1 weather marginal |
| F168 | Exp1 density marginal |
| F169 | Exp1 distance buckets |
| F170 | Exp1 type counts |
| F171 | Exp2 distances 60 points |
| F172 | Exp2 theory slope |
| F173 | Exp2 get P_mlp, P_mamba, P_dual |
| F174 | Exp2 linear fit in log(d) |
| F175 | Exp2 slope error |
| F176 | Exp2 report Rural/Urban |
| F177 | Exp3 fixed d=300, two densities |
| F178 | Exp3 physics, mlp, mamba, dual, trigger |
| F179 | Exp3 physics power formula |
| F180 | Exp3 theory_atten |
| F181 | Exp4 test set 4500 |
| F182 | Exp4 category indices |
| F183 | Exp4 per-sample err (MSE over 5) |
| F184 | Exp4 MSE_mlp per category |
| F185 | Exp4 MSE_mamba, MSE_dual |
| F186 | Exp4 mlp_improvement % |
| F187 | Exp5 latency mean 50 runs |
| F188 | Exp5 params from health |
| F189 | Exp5 D²TL parallel = max(MLP,Mamba) |
| F190 | Exp5 trigger rate 0.15 |
| F191 | Exp5 early-exit T_ee |
| F192 | Exp5 speedup |
| F193 | Exp6 same test set and responses |
| F194 | Exp6 MSE MLP Only |
| F195 | Exp6 MSE Mamba Only |
| F196 | Exp6 MSE Random 50/50 |
| F197 | Exp6 MSE Any-Extreme |
| F198 | Exp6 MSE D²TL Selector |
| F199 | Exp6 MSE Soft Blend |
| F200 | Exp7 per-sample errors |
| F201 | Exp7 mean |
| F202 | Exp7 median |
| F203 | Exp7 percentile |
| F204 | Exp7 tail improvement p95 |
| F205 | Exp7 tail improvement p99, max |
| F206 | Batch size and K batches |
| F207 | POST /predict/batch |
| F208 | Concatenate predictions |
| F209 | Physics constants in experiments |
| F210 | Wavelength and L0 in code |
| F211 | DENSITY_OBSTACLE, WEATHER_ATTEN |
| F212 | ANTENNA_GAIN average |

---

## 10. Appendix B: Experiment Protocol (Step-by-Step)

### 10.1 Prerequisites

- Training data: `coverage_training_data_v2.json` (or v1) with 30,000 samples.
- Trained checkpoints: MLP `best_mlp_coverage.pth`, Mamba `best_coverage.pth`.
- For API experiments: Selector (8000), MLP (8001), Mamba (8002) running.

### 10.2 Exp1 Protocol

1. Load full training dataset.
2. For each sample, compute extreme flag: (weather≥2) OR (distance>500) OR (density≥2) OR (num_interferers≥3).
3. Sum extreme count, normal count; compute extreme_pct.
4. Compute weather marginal: count per w in {0,1,2,3}.
5. Compute density marginal: count per k in {0,1,2,3}.
6. Compute distance buckets: <200, 200–500, 500–800, >800.
7. Compute type counts: heavy_weather, long_distance, dense_urban, high_interference (each from one condition).
8. Write exp1 dict to results JSON.

### 10.3 Exp2 Protocol

1. Define distances = linspace(50, 1000, 60).
2. For each environment (Rural k=0, Urban k=2):
   - Compute theory_slope = -10 * n * η_obs(k).
   - For each distance d_i, build input (distance=d_i, weather=0, density=k, ...); get MLP, Mamba, Dual power (via API or local forward).
   - Stack powers; fit linear regression P = a*log10(d/d0) + b.
   - Store slopes and slope_error_dB = a - theory_slope.
3. Write exp2 dict (distances, Rural_*, Urban_*).

### 10.4 Exp3 Protocol

1. Fix distance = 300 m; densities = [0, 1] (Rural, Suburban); weather = [0,1,2,3].
2. For each (density, weather): build input; get physics (formula), MLP, Mamba, Dual power and trigger_score.
3. Fill table: env → weather_name → {physics, mlp, mamba, dual, trigger, theory_atten}.
4. Write exp3 dict.

### 10.5 Exp4 Protocol

1. Take test set = last 4500 samples (or fixed test split).
2. Classify each index into categories: normal, extreme_weather, extreme_distance, extreme_density, extreme_compound (compound = ≥2 factors).
3. If using API: run batch prediction (57 batches of 80) once; get list of responses (each has mlp_prediction, mamba_prediction, and selected output).
4. For each category, for each index in category: compute err(mlp, true), err(mamba, true), err(dual, true). Average to get mlp_mse, mamba_mse, dual_mse. Compute mlp_improvement.
5. Write exp4 dict.

### 10.6 Exp5 Protocol

1. Single fixed input (e.g. distance=300, weather=0, density=0).
2. For MLP: 50× POST /predict to 8001; record latency each time; mean and std.
3. For Mamba: 50× POST /predict to 8002; same.
4. For Selector: 50× POST /predict to 8000; same.
5. Get parameters from GET /health for 8001 and 8002.
6. Compute D²TL parallel latency = max(MLP_latency, Mamba_latency).
7. Set trigger_rate = 0.15; D²TL early-exit = trigger_rate*parallel + (1-trigger_rate)*MLP_latency.
8. Speedup = Mamba_latency / early-exit.
9. Write exp5 dict.

### 10.7 Exp6 Protocol

1. Use same test set and same batch responses as Exp4.
2. For each sample n: get true, mlp_pred, mamba_pred, dual_pred (Selector output).
3. Compute MSE for: MLP Only (use mlp_pred), Mamba Only (use mamba_pred), Random 50/50 (with seed 42), Any-Extreme (Mamba if extreme else MLP), D²TL Selector (dual_pred), Soft Blend (0.5*mlp + 0.5*mamba).
4. Average over N_test; write exp6 dict.

### 10.8 Exp7 Protocol

1. Same test set and responses.
2. For each sample: err_mlp, err_mamba, err_dual (MSE over 5 outputs).
3. For MLP, Mamba, D²TL: compute mean, median, p90, p95, p99, max of error distribution.
4. tail_improvement: p95_vs_mlp = 100*(1 - D²TL_p95/MLP_p95), similarly p99, max.
5. Write exp7 dict.

---

## 11. Appendix C: Algorithm Pseudocode

### 11.1 Selector Decision (Single Request)

```
function SELECT(input x):
  analysis ← PhysicsAnalyzer.analyze(x)
  mlp_out  ← call_mlp(x)
  mamba_out ← call_mamba(x)
  div ← check_divergence(mlp_out, mamba_out)
  use_mamba ← analysis.use_mamba OR (div.divergent AND analysis.trigger_score ≥ 0.15)
  if use_mamba then return mamba_out else return mlp_out
```

### 11.2 MLP Forward (Single Sample)

```
function MLP_FORWARD(x):
  h ← x
  h ← GELU(LayerNorm(Linear_0(h)))
  for l = 1 to 7:
    h ← Dropout(GELU(LayerNorm(Linear_l(h))))
  y_power  ← head_power(h)
  y_sinr   ← head_sinr(h)
  y_radius ← head_radius(h)
  y_area   ← head_area(h)
  y_qos    ← head_qos(h)
  return [y_power, y_sinr, y_radius, y_area, y_qos]
```

### 11.3 Mamba Forward (Single Sample)

```
function MAMBA_FORWARD(x):
  x ← InputProj(x)        // Linear + LN + GELU + Dropout
  for l = 1 to 8:
    x ← MambaBlock(x)     // LN → in_proj → split → SSM → gate → out_proj + residual
  x ← LayerNorm(x)
  x ← mean(x, dim=seq) or squeeze(seq)
  y ← [head_power(x), head_sinr(x), head_radius(x), head_area(x), head_qos(x)]
  return y
```

### 11.4 Trigger Score (PhysicsAnalyzer)

```
function ANALYZE(x):
  d ← x.distance_to_rx_m
  w ← x.weather_condition
  k ← x.building_density
  nint ← x.num_interferers
  s ← 0
  if d > 700 then s ← s + 0.35
  else if d > 500 then s ← s + 0.20
  if d > 500 and w ≥ 2 then s ← s + 0.20
  else if w ≥ 3 then s ← s + 0.10
  if d > 500 and k ≥ 2 then s ← s + 0.15
  if nint ≥ 3 and d > 400 then s ← s + 0.10
  n_risk ← I(w≥2) + I(d>500) + I(k≥2) + I(nint≥3)
  if n_risk ≥ 3 then s ← s + 0.15
  s ← min(s, 1.0)
  return { trigger_score: s, use_mamba: (s ≥ 0.3), ... }
```

---

## 12. Appendix D: Data Schema and API Contract

### 12.1 Single Sample (Training Data / Generator Output)

- **Inputs (13):** rsu_x_position_m, rsu_y_position_m, tx_power_dbm, antenna_tilt_deg, antenna_azimuth_deg, distance_to_rx_m, angle_to_rx_deg, building_density, weather_condition, vehicle_density_per_km2, num_interferers, rx_height_m, frequency_ghz.
- **Targets (5):** received_power_dbm, sinr_db, coverage_radius_m, coverage_area_km2, qos_score.
- **Auxiliary (optional):** path_loss_db, antenna_gain_db, interference_power_dbm, coverage_probability, throughput_mbps, is_covered, is_good_quality.

### 12.2 Selector POST /predict Request (CoverageInput)

Same 13 fields as above; defaults: distance_to_rx_m=300, building_density=1, weather_condition=0, etc.

### 12.3 Selector POST /predict Response (DualPredictionResult)

- received_power_dbm, sinr_db, coverage_radius_m, coverage_area_km2, qos_score (final selected).
- selected_model (string), trigger_score, risk_level, reasons (list).
- mlp_prediction, mamba_prediction (full dicts), divergence (power_diff_dB, sinr_diff_dB, divergent, note).
- total_inference_ms, mlp_latency_ms, mamba_latency_ms.

### 12.4 Batch POST /predict/batch

- Request: JSON array of CoverageInput (list of 13-field objects).
- Response: { "predictions": [ DualPredictionResult, ... ], "count": N, "mamba_activations": k, "mlp_decisions": N-k }.

---

## 13. Appendix E: Backbone Dimensions (Layer-by-Layer)

### 13.1 MLP

| Layer | Op | In dim | Out dim | Params (approx) |
|-------|-----|--------|---------|------------------|
| 0 | Linear | 13 | 256 | 13*256+256 = 3584 |
| 0 | LayerNorm | 256 | 256 | 512 |
| 1–7 | Linear | 256 | 256 | 65792 each |
| 1–7 | LayerNorm | 256 | 256 | 512 each |
| heads | Linear | 256 | 1 | 257 each × 5 = 1285 |
| **Total** | | | | **~469K** |

### 13.2 Mamba-3

| Stage | Op | In dim | Out dim |
|-------|-----|--------|---------|
| Input | Linear | 13 | 256 |
| Input | LayerNorm + GELU + Dropout | 256 | 256 |
| Block 1–8 | MambaBlock (in_proj 256→1024, split→512 each, SSM 512, out_proj 512→256) | 256 | 256 |
| Final | LayerNorm | 256 | 256 |
| Pool | mean/squeeze | (B,1,256) | (B,256) |
| Heads | 5 × Linear(256→1) | 256 | 5 |
| **Total params** | | | **~13.7M** |

### 13.3 SelectiveSSM (per block, d_inner=512)

| Op | Shape / Params |
|----|-----------------|
| Conv1d | 512, kernel 4, groups 512 |
| x_proj | Linear 512 → 1536 (Δ,B,C each 512) |
| dt_proj | Linear 512 → 512 |
| A_log | 512×16 |
| D | 512 |
| out_proj | Linear 512 → 512 |

---

---

## 14. Appendix F: Constants Quick Reference

| Symbol | Value | Unit | Usage |
|--------|--------|------|--------|
| \(c\) | 3×10⁸ | m/s | Speed of light |
| \(f_c\) | 5.9 | GHz | Carrier frequency |
| \(\lambda\) | ≈0.0508 | m | Wavelength |
| \(P_{\mathrm{tx}}\) | 33 | dBm | Nominal TX power |
| \(N_0\) | -95 | dBm | Noise floor |
| \(B\) | 10 | MHz | Bandwidth |
| \(n\) | 3.5 | — | Path-loss exponent |
| \(d_0\) | 1.0 | m | Reference distance |
| \(\sigma_{\mathrm{sh}}\) | 8.0 | dB | Shadowing std |
| \(G_{\max}\) | 10.0 | dBi | Max antenna gain |
| \(\theta_{\mathrm{bw}}\) | 120 | ° | Beamwidth |
| \(P_{\min}\) | -90 | dBm | RX sensitivity |
| \(\gamma_{\min}\) | -5 | dB | Min SINR |
| \(\eta_{\mathrm{obs}}(0)\) | 1.0 | — | Rural |
| \(\eta_{\mathrm{obs}}(1)\) | 1.2 | — | Suburban |
| \(\eta_{\mathrm{obs}}(2)\) | 1.5 | — | Urban |
| \(\eta_{\mathrm{obs}}(3)\) | 2.0 | — | Ultra-dense |
| \(A_{\mathrm{weather}}(0)\) | 0 | dB | Clear |
| \(A_{\mathrm{weather}}(1)\) | 2 | dB | Light rain |
| \(A_{\mathrm{weather}}(2)\) | 5 | dB | Moderate rain |
| \(A_{\mathrm{weather}}(3)\) | 8 | dB | Heavy rain |
| \(d_{\mathrm{th}}\) | 500 | m | Selector long range |
| \(d_{\mathrm{th2}}\) | 700 | m | Selector very long |
| \(\Delta_{\mathrm{div}}\) | 5 | dB | Divergence threshold |
| MLP hidden | 256 | — | Hidden size |
| MLP layers | 8 | — | Depth |
| Mamba d_model | 256 | — | Model dim |
| Mamba n_layers | 8 | — | Blocks |
| Mamba d_state | 16 | — | SSM state |
| Mamba d_conv | 4 | — | Conv kernel |
| Mamba expand | 2 | — | d_inner = 512 |
| Train size | 21000 | — | Training samples |
| Val size | 4500 | — | Validation |
| Test size | 4500 | — | Test |
| Batch size | 64 | — | Training batch |
| Batch size API | 80 | — | Exp batch |
| Loss weight power | 0.15 | — | Multi-task |
| Loss weight sinr | 0.15 | — | |
| Loss weight radius | 0.30 | — | |
| Loss weight area | 0.30 | — | |
| Loss weight qos | 0.10 | — | |
| Trigger threshold | 0.3 | — | use_mamba |
| Trigger tiebreaker | 0.15 | — | divergence |
| Trigger rate (Exp5) | 0.15 | — | Early-exit fraction |

---

## 15. Storyline (Extended)

### 15.1 Problem

In 6G V2X, the RSU must predict coverage (received power, SINR, coverage radius/area, QoS) for many receivers under varying geometry, weather, and density. Two competing goals:

- **Latency:** Sub-10 ms for real-time beamforming and handover.
- **Accuracy under extrapolation:** Long distance, heavy rain, and dense urban break simple empirical models; physics (Friis, ITU-R rain) matters.

### 15.2 Design Choice: Dual Path

Instead of one big model, we use:

1. **MLP (primary):** Trained on the same data as Mamba; fast (1–7 ms); excellent R² on in-distribution data; can drift under extreme conditions.
2. **Mamba-3 (backup):** State-space model with selective mechanism; learns smooth, physics-consistent behavior (path-loss slope, rain attenuation); slower (~60 ms) but reliable when MLP would err.
3. **Selector:** No trainable parameters; rule-based trigger (distance, weather, density, interferers) plus divergence check; chooses MLP or Mamba per request.

### 15.3 Data Story

Data is generated by a **physics-based simulator** (CoverageDataGeneratorV2) so that:

- Path loss follows log-distance with density-dependent exponent (F7–F9).
- Weather adds fixed attenuation 0/2/5/8 dB (F6, F13).
- Antenna gain follows cosine-squared in beam (F11–F12).
- Interference and SINR are computed from multiple interferers (F14–F23).
- Coverage radius is derived from reverse path loss and thresholds (F24–F36).
- QoS aggregates SINR, power, distance, and coverage probability (F41–F45).

This gives 30,000 samples with known physics; both MLP and Mamba are trained to fit these targets.

### 15.4 Training Story

- **Same normalization:** Both models use the same feature mean/std (from Mamba checkpoint or dataset).
- **Same target scaling:** Power, SINR, radius, area, QoS scaled to [0,1] (F135–F140).
- **Same loss:** Weighted MSE plus small penalties for radius/area/QoS bounds (F141–F147).
- **Different optimizers/schedules:** MLP uses AdamW + cosine; Mamba uses AdamW + 3-stage (warmup, stable, decay). Both use gradient clipping.

Result: MLP reaches R² ≈ 0.93 with ~469K params; Mamba ~13.7M params with similar or slightly better physics consistency on extremes.

### 15.5 Inference Story

- Every request hits the **Selector** (port 8000).
- Selector calls **MLP** (8001) and **Mamba** (8002) in parallel.
- PhysicsAnalyzer computes **trigger_score** from distance, weather, density, interferers (F111–F121).
- If score ≥ 0.3 → use Mamba.
- Else if models **diverge** (|ΔP| or |Δγ| > 5 dB) and score ≥ 0.15 → use Mamba.
- Otherwise → use MLP.

So in “normal” conditions the system returns MLP (low latency); in “extreme” or disagreeing conditions it returns Mamba (physics-aware).

### 15.6 Experiment Story

- **Exp1** describes the data (how many extreme vs normal, marginals).
- **Exp2** checks that Dual/MLP/Mamba power vs distance has the right slope trend (theory: −10·n_eff dB per decade).
- **Exp3** shows power vs weather at 300 m (Rural/Suburban); trigger and dual vs physics.
- **Exp4** shows that D²TL (dual) improves over MLP in extreme categories (stratified MSE).
- **Exp5** measures latency and speedup (early-exit vs Mamba-only).
- **Exp6** compares six strategies (MLP only, Mamba only, random, any-extreme, D²TL, soft blend).
- **Exp7** quantifies tail risk (p95, p99, max error) and improvement of D²TL over MLP.

Together, these experiments justify the dual-path design and the Selector logic.

---

## 16. Appendix G: Formula Derivations & Notes

### 16.1 Free-Space Path Loss (F2, F3)

The free-space path loss at distance \(d\) is \(L = (4\pi d / \lambda)^2\) in linear scale. In dB:
\[
L_{\mathrm{dB}} = 10\log_{10}(L) = 20\log_{10}(4\pi d / \lambda) = 20\log_{10}(4\pi/\lambda) + 20\log_{10}(d).
\]
At \(d = d_0 = 1\) m, \(L_0 = 20\log_{10}(4\pi/\lambda)\). With \(\lambda = c/f = 3\times10^8/(5.9\times10^9) \approx 0.0508\) m, \(4\pi/\lambda \approx 247.2\), so \(L_0 \approx 47.86\) dB.

### 16.2 Log-Distance Path Loss (F7)

Empirical model: \(P_{\mathrm{rx}} \propto d^{-n_{\mathrm{eff}}}\). In dB:
\[
\mathrm{PL}_{\mathrm{log}}(d) = L_0 + 10\,n_{\mathrm{eff}}\,\log_{10}(d/d_0).
\]
Slope in dB per decade of distance: \(10\,n_{\mathrm{eff}}\) (e.g. \(n_{\mathrm{eff}}=3.5\) → 35 dB/decade).

### 16.3 SINR in dB (F22–F23)

\(\mathrm{SINR}^{\mathrm{lin}} = S/(I+N)\). In dB: \(\gamma = 10\log_{10}(\mathrm{SINR}^{\mathrm{lin}})\). When \(I=0\), \(\gamma = P_{\mathrm{rx}} - N_0\) (dB).

### 16.4 Coverage Radius from Max Path Loss (F29–F32)

Max path loss allowed: \(\mathrm{PL}_{\max} = P_{\mathrm{tx}} + G - P_{\min,\mathrm{eff}} - A_{\mathrm{weather}} - M_{\mathrm{int}}\). Solve for \(d\):
\[
L_0 + 10\,n_{\mathrm{eff}}\,\log_{10}(d/d_0) = \mathrm{PL}_{\max} \Rightarrow \log_{10}(d) = \frac{\mathrm{PL}_{\max} - L_0}{10\,n_{\mathrm{eff}}} + \log_{10}(d_0).
\]
Then apply shadowing margin and clip to [200, 1000] m for realism.

### 16.5 LayerNorm (F61–F62)

For vector \(\mathbf{z}\) of dimension \(H\): \(\mu = \frac{1}{H}\sum_i z_i\), \(\sigma^2 = \frac{1}{H}\sum_i (z_i - \mu)^2 + \epsilon\). Output \(\hat{z}_i = \gamma_i \frac{z_i - \mu}{\sqrt{\sigma^2}} + \beta_i\). In training we use the same statistics per feature dimension across the batch.

### 16.6 GELU (F63)

Gaussian Error Linear Unit: \(\mathrm{GELU}(x) = x\,\Phi(x)\) where \(\Phi\) is the standard normal CDF. Approximate: \(0.5x(1+\tanh(\sqrt{2/\pi}(x + 0.044715 x^3)))\). Used for smooth gradients.

### 16.7 Trigger Score Bounds (F120, F121)

Score \(s\) is cumulative from rules (F112–F119); maximum possible is 0.35+0.20+0.20+0.15+0.10+0.15 = 1.15 before clamp. After \(\min(s, 1)\), \(s \in [0, 1]\). use_mamba = (s ≥ 0.3) is calibrated so that only clearly extreme or compound scenarios activate Mamba.

### 16.8 Multi-Task Loss Weights (F142)

Radius and area get 0.30 each because they are harder (non-linear in inputs); power and SINR get 0.15 each; QoS gets 0.10. Sum = 1.0. Penalties (F143–F146) are small (0.02–0.05) to avoid negative or out-of-range predictions.

### 16.9 R² Definition (F154–F157)

\(R^2 = 1 - \mathrm{SS}_{\mathrm{res}}/\mathrm{SS}_{\mathrm{tot}}\) where SS_res = sum of squared residuals and SS_tot = total sum of squares. \(R^2 \le 1\); 1 means perfect fit. Per-target R² then averaged for overall.

### 16.10 Early-Exit Latency (F191)

If a fraction \(r\) of requests use Mamba (trigger rate), and we assume Mamba and MLP run in parallel when both are called, effective latency is \(r \cdot \max(T_{\mathrm{mlp}}, T_{\mathrm{mamba}}) + (1-r) \cdot T_{\mathrm{mlp}}\). With \(r=0.15\) and \(T_{\mathrm{mamba}} \gg T_{\mathrm{mlp}}\), this is approximately \(0.15\,T_{\mathrm{mamba}} + 0.85\,T_{\mathrm{mlp}}\).

---

## 17. Appendix H: Code-to-Formula Cross-Reference

| Formula(s) | Code location (file: approximate location) |
|------------|--------------------------------------------|
| F1–F3 | `generate_coverage_data_v2.py`: wavelength, L0 in _calculate_path_loss |
| F4–F6 | `generate_coverage_data_v2.py`: obstacle_factor dict, weather_attenuation dict |
| F7–F9 | `generate_coverage_data_v2.py`: _calculate_path_loss |
| F10–F12 | `generate_coverage_data_v2.py`: _calculate_antenna_gain |
| F13 | `generate_coverage_data_v2.py`: received_power_dbm = tx + gain - path_loss - weather |
| F14–F19 | `generate_coverage_data_v2.py`: _calculate_interference |
| F20–F23 | `generate_coverage_data_v2.py`: _calculate_sinr |
| F24–F36 | `generate_coverage_data_v2.py`: _calculate_coverage_radius, sinr_factor, environment_factor |
| F37–F48 | `generate_coverage_data_v2.py`: coverage_area_km2, _sinr_to_coverage_prob, _calculate_qos, _estimate_throughput |
| F49–F59 | `generate_coverage_data_v2.py`: _generate_single_sample (sampling) |
| F60–F66 | `d2tl/mlp_service/model.py`: CoverageMLP.forward, backbone layers |
| F67–F73 | `d2tl/mlp_service/model.py`: head_power, head_sinr, etc., torch.cat |
| F74–F76 | `d2tl/mlp_service/model.py`: _init_weights |
| F77–F80 | `d2tl/mlp_service/model.py`: get_num_params |
| F81–F82 | `models/mamba3_coverage.py`: CoverageMamba3.input_proj |
| F83–F90 | `models/mamba3_coverage.py`: MambaBlock.forward |
| F91–F102 | `models/mamba3_coverage.py`: SelectiveSSM.forward, _stable_ssm |
| F103–F109 | `models/mamba3_coverage.py`: CoverageMamba3.forward, get_num_params |
| F110–F121 | `d2tl/selector_brain/selector.py`: PhysicsAnalyzer.analyze, constants |
| F122–F127 | `d2tl/selector_brain/selector.py`: check_divergence, use_mamba decision |
| F128–F131 | `d2tl/selector_brain/selector.py`: risk_level, predict_batch |
| F132–F140 | `d2tl/training/train_mlp.py`, `training/train_coverage.py`: CoverageDataset |
| F141–F147 | `d2tl/training/train_mlp.py`, `training/train_coverage.py`: multi_task_loss |
| F148–F153 | `d2tl/training/train_mlp.py`: AdamW, CosineAnnealingLR, clip_grad_norm_ |
| F150–F152 | `training/train_coverage.py`: get_lr_schedule |
| F154–F161 | `d2tl/training/train_mlp.py`, `training/train_coverage.py`: compute_metrics, split |
| F162–F212 | `d2tl/experiments/run_all_experiments.py`, `run_all_experiments_via_api.py`: exp1–exp7, batch |

---

## 18. Appendix I: Experiment Results JSON Schema

The file `all_experiment_results_via_api.json` (or `all_experiment_results.json`) has the following structure. Every key is produced by the formulas in Part VI.

### 18.1 exp1

```json
{
  "exp1": {
    "total": 30000,
    "extreme": <F164>,
    "normal": <F165>,
    "extreme_pct": <F166>,
    "weather": { "Clear": <F167>, "Light Rain": ..., "Moderate Rain": ..., "Heavy Rain": ... },
    "density": { "Rural": ..., "Suburban": ..., "Urban": ..., "Ultra-Dense": ... },
    "distance": { "<200": ..., "200-500": ..., "500-800": ..., ">800": ... },
    "types": { "dense_urban": ..., "heavy_weather": ..., "long_distance": ..., "high_interference": ... }
  }
}
```

### 18.2 exp2

```json
{
  "exp2": {
    "distances": [<F171>],
    "Rural_mlp_slope": <F174>,
    "Rural_mlp_slope_error_dB": <F175>,
    "Rural_mamba_slope": ...,
    "Rural_mamba_slope_error_dB": ...,
    "Rural_dual_slope": ...,
    "Rural_dual_slope_error_dB": ...,
    "Rural_theory_slope": <F172>,
    "Urban_mlp_slope": ...,
    "Urban_mlp_slope_error_dB": ...,
    "Urban_mamba_slope": ...,
    "Urban_mamba_slope_error_dB": ...,
    "Urban_dual_slope": ...,
    "Urban_dual_slope_error_dB": ...,
    "Urban_theory_slope": ...
  }
}
```

### 18.3 exp3

```json
{
  "exp3": {
    "Rural": {
      "Clear": { "physics": <F179>, "mlp": ..., "mamba": ..., "dual": ..., "trigger": ..., "theory_atten": <F180> },
      "Light Rain": { ... },
      "Moderate Rain": { ... },
      "Heavy Rain": { ... }
    },
    "Suburban": { "Clear": { ... }, ... }
  }
}
```

### 18.4 exp4

```json
{
  "exp4": {
    "normal": { "n": <count>, "mlp_mse": <F184>, "mamba_mse": <F185>, "dual_mse": <F185>, "mlp_improvement": <F186> },
    "extreme_weather": { ... },
    "extreme_distance": { ... },
    "extreme_density": { ... },
    "extreme_compound": { ... }
  }
}
```

### 18.5 exp5

```json
{
  "exp5": {
    "MLP": { "latency_ms": <F187>, "latency_std": ..., "params": <F188> },
    "Mamba-3": { "latency_ms": ..., "latency_std": ..., "params": ... },
    "D²TL (parallel)": { "latency_ms": <F189>, "note": "..." },
    "D²TL (early-exit)": { "latency_ms": <F191>, "trigger_rate": <F190>, "note": "..." },
    "speedup": <F192>
  }
}
```

### 18.6 exp6

```json
{
  "exp6": {
    "MLP Only": <F194>,
    "Mamba Only": <F195>,
    "Random 50/50": <F196>,
    "Any-Extreme Trigger": <F197>,
    "D²TL Selector": <F198>,
    "Soft Blend (0.5)": <F199>
  }
}
```

### 18.7 exp7

```json
{
  "exp7": {
    "MLP": { "mean": <F201>, "median": <F202>, "p90": <F203>, "p95": ..., "p99": ..., "max": ... },
    "Mamba": { ... },
    "D²TL": { ... },
    "tail_improvement": { "p95_vs_mlp": <F204>, "p99_vs_mlp": <F205>, "max_vs_mlp": <F205> }
  }
}
```

---

## 19. Appendix J: Narrative Timeline (Chronological)

1. **Data generation**  
   Run `generate_coverage_data_v2.py`. Uses F1–F59. Output: 30,000 samples in JSON.

2. **Feature statistics**  
   From dataset (or Mamba checkpoint): compute μ, σ (F132–F133). Used for normalization (F134).

3. **Target scaling**  
   Apply F135–F140 to all targets. Store scaled targets in dataset.

4. **Train Mamba-3**  
   Load data, build CoverageMamba3 (F81–F109). Train with F141–F147, F150–F153, F148 (AdamW). Validate with F154–F159. Save best_coverage.pth and feature_stats.

5. **Train MLP**  
   Load same data and feature_stats. Build CoverageMLP (F60–F80). Train with F141–F147, F148–F149, F153. Validate with F154–F159. Save best_mlp_coverage.pth.

6. **Deploy services**  
   Start MLP (8001), Mamba (8002), Selector (8000). Selector uses F110–F131.

7. **Run experiments**  
   - Exp1: F162–F170 on training data.  
   - Exp2: F171–F176 via API or local.  
   - Exp3: F177–F180 via API or local.  
   - Exp4: F181–F186, F206–F208 (batch).  
   - Exp5: F187–F192.  
   - Exp6: F193–F199 (same batch as Exp4).  
   - Exp7: F200–F205 (same batch).

8. **Save results**  
   Write all_experiment_results_via_api.json (or all_experiment_results.json) with exp1–exp7. Copy to paper_package and GITHUB/data.

9. **Validation**  
   Run validate_experiment_data.py to check consistency (Exp1 sums, Exp2 slopes, Exp4 dual_mse ≤ mlp_mse when improvement > 0, Exp6 D²TL ≤ MLP, Exp7 D²TL mean = Exp6 D²TL Selector).

10. **Visualization**  
    Run plot_all_training_and_experiments.py to generate training curves and experiment plots from the JSON and training history files.

---

## 20. Appendix K: Summary Checklist for Paper / Report

- [ ] **Abstract:** D²TL = dual-path (MLP + Mamba-3) + Selector; 6G RSU coverage; low latency + physics-aware backup.
- [ ] **Introduction:** Motivation (latency vs extrapolation); contribution (selector logic, seven experiments).
- [ ] **Related work:** MLP for coverage; state-space models (Mamba); hybrid systems.
- [ ] **System model:** 13 inputs, 5 outputs; physics (path loss, weather, interference, SINR, radius, QoS).
- [ ] **Data generation:** Reference F1–F59 and data generator; 30K samples.
- [ ] **MLP backbone:** F60–F80; 8 layers, LayerNorm, GELU, 5 heads; ~469K params.
- [ ] **Mamba-3 backbone:** F81–F109; SelectiveSSM, MambaBlock, 8 layers; ~13.7M params.
- [ ] **Selector:** F110–F131; trigger score, divergence, decision rule.
- [ ] **Training:** F132–F161; same normalization and loss for both models.
- [ ] **Experiments:** Exp1–Exp7 with F162–F212; report tables/figures from JSON.
- [ ] **Results:** Distribution (Exp1), slope consistency (Exp2), rain (Exp3), stratified MSE (Exp4), latency/speedup (Exp5), ablation (Exp6), tail risk (Exp7).
- [ ] **Conclusion:** D²TL achieves low latency with physics backup; Selector activates Mamba only when needed; experiments validate design.

---

## 21. Appendix L: Expected Outcomes and Interpretation

### Exp1 (Distribution)

- **Expected:** total = 30,000; extreme_pct typically 70–85% (data is biased toward long distance, urban, or rain); weather/density/distance marginals sum to total; normal + extreme = total.
- **Interpretation:** The dataset is “extreme-rich,” so the Selector’s Mamba path is exercised on a sizable fraction of samples in deployment if traffic is similar.

### Exp2 (Distance–Power)

- **Expected:** All slopes (MLP, Mamba, Dual) negative; theory slope Rural −35, Urban −52.5 (dB per decade); Dual slope usually between MLP and Mamba and often closer to theory on long range; Mamba slope error can be smaller than MLP in Urban.
- **Interpretation:** Mamba’s state-space structure helps it track the log-distance law; Selector (Dual) inherits this when it chooses Mamba for long range.

### Exp3 (Rainstorm)

- **Expected:** Power decreases (more negative dBm) from Clear to Heavy Rain; physics and Mamba often agree better on attenuation (≈8 dB for Heavy Rain); Dual equals MLP or Mamba; trigger_score increases with weather severity.
- **Interpretation:** Under rain, Mamba’s learned attenuation aligns with ITU-R; Selector can switch to Mamba when trigger ≥ 0.3.

### Exp4 (Stratified)

- **Expected:** dual_mse ≤ mlp_mse in extreme_weather, extreme_distance, extreme_density, extreme_compound; mlp_improvement > 0 in those categories; in normal, dual_mse = mlp_mse (Selector uses MLP).
- **Interpretation:** D²TL improves over MLP exactly where extremes dominate; no loss in normal.

### Exp5 (Cost)

- **Expected:** MLP latency ~1–7 ms; Mamba ~60–80 ms; D²TL parallel ≈ max(MLP, Mamba); early-exit latency much lower than Mamba-only; speedup > 1 (e.g. 5–6×).
- **Interpretation:** Early-exit gives a good latency–accuracy tradeoff.

### Exp6 (Ablation)

- **Expected:** D²TL Selector ≤ MLP Only (MSE); Mamba Only often worse (higher MSE) on this test set; Random 50/50 and Soft Blend in between; Any-Extreme can be close to D²TL if trigger logic matches.
- **Interpretation:** The learned Selector (rule-based) performs at least as well as MLP and can beat naive strategies.

### Exp7 (Tail Risk)

- **Expected:** D²TL mean = Exp6 D²TL Selector (same errors); tail_improvement (p95, p99, max) can be 0 if Selector chose MLP for tail samples, or positive if Mamba improved those.
- **Interpretation:** Tail improvement is scenario-dependent; when Mamba is chosen for worst-case inputs, D²TL reduces tail error.

---

## 22. Appendix M: Symbol and Term Glossary

| Symbol / Term | Meaning |
|---------------|---------|
| \(d\) | Distance to receiver (m) |
| \(d_0\) | Reference distance (1 m) |
| \(f_c\) | Carrier frequency (GHz) |
| \(\lambda\) | Wavelength (m) |
| \(P_{\mathrm{tx}}\) | Transmit power (dBm) |
| \(P_{\mathrm{rx}}\) | Received power (dBm) |
| \(G\) | Antenna gain (dBi) |
| \(\mathrm{PL}\) | Path loss (dB) |
| \(L_0\) | Free-space path loss at \(d_0\) (dB) |
| \(n\) | Path-loss exponent |
| \(n_{\mathrm{eff}}\) | Effective path-loss exponent (× obstacle factor) |
| \(\eta_{\mathrm{obs}}\) | Obstacle factor (density-dependent) |
| \(A_{\mathrm{weather}}\) | Weather attenuation (dB) |
| \(\gamma\) | SINR (dB) |
| \(\mathrm{SINR}^{\mathrm{lin}}\) | SINR linear scale |
| \(N_0\) | Noise floor (dBm) |
| \(R_{\mathrm{cov}}\) | Coverage radius (m) |
| \(A_{\mathrm{cov}}\) | Coverage area (km²) |
| QoS | Quality of service score (0–100) |
| \(\mathbf{x}\) | Input feature vector (13-dim) |
| \(\tilde{\mathbf{x}}\) | Normalized input |
| \(\mathbf{h}^{(l)}\) | Hidden state at layer \(l\) |
| \(\mathbf{y}\) | Output vector (5-dim: power, sinr, radius, area, qos) |
| \(\mathcal{L}\) | Loss function |
| \(R^2\) | Coefficient of determination |
| MAE | Mean absolute error |
| MSE | Mean squared error |
| trigger_score | Selector risk score [0, 1] |
| use_mamba | Boolean: return Mamba output |
| divergent | MLP vs Mamba difference > threshold |
| D²TL | Dual-Path with Physics-Aware Trigger Logic |
| RSU | Road Side Unit |
| V2X | Vehicle-to-everything |
| ITU-R | International Telecommunication Union – Radiocommunication |
| Friis | Free-space path loss law |
| LayerNorm | Layer normalization |
| GELU | Gaussian Error Linear Unit |
| SiLU | Sigmoid Linear Unit |
| SelectiveSSM | Selective State Space Model (Mamba core) |
| MambaBlock | One block of Mamba (norm → proj → SSM → gate → out + residual) |

---

*End of document. Total formulas: 212; narrative, backbones, Selector logic, training, experiment design, protocol, pseudocode, schema, dimensions, constants, extended storyline, derivations, code cross-reference, results JSON schema, narrative timeline, paper checklist, expected outcomes, and glossary are fully specified. Line count: 1700+.*
